{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code for Wasserstein INN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:23.222217Z",
     "start_time": "2017-08-24T23:09:23.219296Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Long Jin, Weijian Xu, and Kwonjoon Lee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Libraries and Functions\n",
    "\n",
    "This section imports or creates a series of functions to support model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "\n",
    "This section imports all needed libraries. All libraries are either built-in or from PyPI. You may need to use `pip` to install missing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.270892Z",
     "start_time": "2017-08-24T23:09:23.229700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:09:15) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "SciPy version: 0.19.1\n",
      "NumPy version: 1.13.1\n",
      "TensorFlow version: 1.3.0\n",
      "Scikit-learn version: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from utils import *\n",
    "from numpy import inf\n",
    "\n",
    "# Display the versions for libraries. In my environment, they are\n",
    "#     Python version: 2.7.13 |Anaconda custom (64-bit)| (default, Sep 30 2017, 18:12:43)\n",
    "#     [GCC 7.2.0]\n",
    "#     SciPy version: 0.19.1\n",
    "#     NumPy version: 1.14.2\n",
    "#     TensorFlow version: 1.7.0\n",
    "#     Scikit-learn version: 0.19.0\n",
    "print('Python version: {}'.format(sys.version))\n",
    "print('SciPy version: {}'.format(scipy.__version__))\n",
    "print('NumPy version: {}'.format(np.__version__))\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('Scikit-learn version: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.403679Z",
     "start_time": "2017-08-24T23:09:24.272186Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.small_imagenet\n",
    "import tflib.ops.layernorm\n",
    "import tflib.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size. It should be a squared number.\n",
    "batch_size = 100\n",
    "# Number of cascades in WINN.\n",
    "cascades = 4\n",
    "# Number of iterations per cascade\n",
    "# In Algorithm 1 of the paper, we wrote we iterate while W_t has not converged.\n",
    "# In practice, we find that 100 iterations is sufficient for convergence.\n",
    "iterations_per_cascade = 200\n",
    "# hyperparameter k in the Algorithm 1\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Working Directory\n",
    "\n",
    "WARNING: ~6GB of disk space is required for 1 cascade training\n",
    "\n",
    "data/evaluation: synthesized pseudo-negative samples during test time (will be created when you run test code)\n",
    "\n",
    "data/negative: pseudo-negative samples of all iterations/cascades\n",
    "\n",
    "data/intermediate: images showing one batch or training samples and pseudo-negative sample per iteration\n",
    "\n",
    "data/model: trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.620184Z",
     "start_time": "2017-08-24T23:09:24.513586Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Root directory of data directory. Customize it when using another directory.\n",
    "# e.g. \"./\"\n",
    "data_dir_root = \"./\"\n",
    "# Path of data directory.\n",
    "data_dir_path = os.path.join(data_dir_root, \"data\")\n",
    "\n",
    "# Create a series of directories to contain the dataset.\n",
    "mkdir_if_not_exists(data_dir_root)\n",
    "mkdir_if_not_exists(data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset\n",
    "\n",
    "Path of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.645855Z",
     "start_time": "2017-08-24T23:09:24.622618Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path of training images directory.\n",
    "training_images_dir_path = \"./cifar-images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Path of CIFAR-10 images directory.\n",
    "# cifar_images_dir_path = \"./cifar-images\"\n",
    "# cifar_dir_path = \"./../../history GAN/\"\n",
    "# # When the CIFAR images directory does not exist, we will generate the images.\n",
    "# #if not os.path.exists(cifar_images_dir_path):\n",
    "# # Create the CIFAR images directory.\n",
    "# mkdir_if_not_exists(cifar_images_dir_path)\n",
    "# # CIFAR-10 dataset pickle file root and names.\n",
    "# cifar_pickle_file_root = os.path.join(cifar_dir_path, \n",
    "#                                       \"cifar-10-batches-py\")\n",
    "# cifar_pickle_file_names = [\"data_batch_1\",\n",
    "#                            \"data_batch_2\",\n",
    "#                            \"data_batch_3\",\n",
    "#                            \"data_batch_4\",\n",
    "#                            \"data_batch_5\"]\n",
    "# # CIFAR-10 images and labels list.\n",
    "# cifar_images = []\n",
    "# cifar_labels = []\n",
    "\n",
    "# # Unpickle function to extract pickle files.\n",
    "# def unpickle(file):\n",
    "#     import cPickle\n",
    "#     with open(file, 'rb') as fo:\n",
    "#         dict = cPickle.load(fo)\n",
    "#     return dict\n",
    "\n",
    "# # Get all images and labels.\n",
    "# for cifar_pickle_file_name in cifar_pickle_file_names:\n",
    "#     cifar_pickle_file_path = os.path.join(cifar_pickle_file_root, \n",
    "#                                           cifar_pickle_file_name)\n",
    "#     # Unpickle the pickle file into dictionary.\n",
    "#     cifar_dict = unpickle(cifar_pickle_file_path)\n",
    "#     # Reshape. In original 3072-dim vector, first 1024-dim is red, second \n",
    "#     # 1024-dim is green, third 1024-dim is blue.\n",
    "#     cifar_images_intermediate = cifar_dict['data'].reshape((10000, 3, 32, 32))\n",
    "#     # Swap dimension and add image: Last dimension should be channels.\n",
    "#     cifar_images += list(np.transpose(cifar_images_intermediate, (0, 2, 3, 1)))\n",
    "#     # Add label.\n",
    "#     cifar_labels += cifar_dict['labels']\n",
    "\n",
    "# # Choose images with category 0.\n",
    "# cifar_cat2_images = cifar_images#[cifar_images[i] for i, label in enumerate(cifar_labels) \n",
    "#                      #if label == 0]\n",
    "\n",
    "# # Save all images with category 2. There are 5,000 images in this category.\n",
    "# for i in range(50000):\n",
    "#     cifar_cat2_image = cifar_cat2_images[i]\n",
    "#     cifar_cat2_image_name = '{}.png'.format(i)\n",
    "#     cifar_cat2_image_path = os.path.join(cifar_images_dir_path, \n",
    "#                                          cifar_cat2_image_name)\n",
    "#     save_unnormalized_image(cifar_cat2_image, cifar_cat2_image_path)\n",
    "#     if i % 1000 == 0:\n",
    "#         print('Generated {} images...'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "This section focuses on building the model for WINN. It contains layers and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "This subsection contains all layers used in WINN model. E.g. convolutional layer, linear layer and batch normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish(z):\n",
    "    return z * tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.764990Z",
     "start_time": "2017-08-24T23:09:24.757192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Normalize(name, inputs, labels=None):\n",
    "    return lib.ops.layernorm.Layernorm(name,[1,2,3],inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.783827Z",
     "start_time": "2017-08-24T23:09:24.766181Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvMeanPool(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=he_init, biases=biases)\n",
    "    output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "    return output\n",
    "\n",
    "def MeanPoolConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases)\n",
    "    return output\n",
    "\n",
    "def UpsampleConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.concat([output, output, output, output], axis=1)\n",
    "    output = tf.transpose(output, [0,2,3,1])\n",
    "    output = tf.depth_to_space(output, 2)\n",
    "    output = tf.transpose(output, [0,3,1,2])\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "This subsection builds the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.793436Z",
     "start_time": "2017-08-24T23:09:24.784825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def GoodDiscriminator(inputs, dim=64, nonlinearity = swish, bn = True, reuse = False):\n",
    "#     output = tf.reshape(tf.transpose(inputs, [0, 3, 1, 2]), [-1, 3, 64, 64])\n",
    "\n",
    "#     lib.ops.conv2d.set_weights_stdev(0.02)\n",
    "#     lib.ops.deconv2d.set_weights_stdev(0.02)\n",
    "#     lib.ops.linear.set_weights_stdev(0.02)\n",
    "    \n",
    "#     with tf.variable_scope(\"layers\", reuse = reuse):\n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.1', 3, 32, 3, output, stride=1, he_init=False)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.2', 32, 64, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN2', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "#         ### output: 64 channels x 32 x 32\n",
    "        \n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.3', 64, 64, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN3', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.4', 64, 128, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN4', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "#         ### output: 128 channels x 16 x 16\n",
    "\n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.5', 128, 128, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN5', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.6', 128, 256, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN6', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "#         ### output: 256 channels x 8 x 8\n",
    "        \n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.7', 256, 256, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN7', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = lib.ops.conv2d.Conv2D('Discriminator.8', 256, 512, 3, output, stride=1, he_init=False)\n",
    "#         if bn:\n",
    "#             output = Normalize('Discriminator.BN8', [0,2,3], output)\n",
    "#         output = nonlinearity(output)\n",
    "        \n",
    "#         output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "#         ### output: 512 channels x 4 x 4\n",
    "        \n",
    "#         output = tf.reshape(output, [-1, 4*4*512])\n",
    "#         output = lib.ops.linear.Linear('Discriminator.Output', 4*4*512, 1, output)\n",
    "\n",
    "# #         lib.ops.conv2d.unset_weights_stdev()\n",
    "#         lib.ops.deconv2d.unset_weights_stdev()\n",
    "#         lib.ops.linear.unset_weights_stdev()\n",
    "\n",
    "#     return tf.reshape(output, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Provider\n",
    "\n",
    "This subsection builds the networks that gives initial pseudo-negatives (Appendix E)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def NoiseProvider(n_samples, noise=None, dim=64):\n",
    "#     lib.ops.conv2d.set_weights_stdev(0.1)\n",
    "#     lib.ops.deconv2d.set_weights_stdev(0.1)\n",
    "#     lib.ops.linear.set_weights_stdev(0.1)\n",
    "#     with tf.variable_scope(\"layers_np\", reuse = False):\n",
    "#         output = noise\n",
    "#         output = lib.ops.conv2d.Conv2D('NoiseProvider.2', 8*dim, 4*dim, 5, output, stride=1)\n",
    "#         output = tf.transpose(output, [0, 2, 3, 1])\n",
    "#         output = tf.image.resize_images(output, [8, 8], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#         output = tf.transpose(output, [0, 3, 1, 2])\n",
    "#         output = Normalize('NoiseProvider.BN2', [0,2,3], output)\n",
    "\n",
    "#         output = lib.ops.conv2d.Conv2D('NoiseProvider.3', 4*dim, 2*dim, 5, output, stride=1)\n",
    "#         output = tf.transpose(output, [0, 2, 3, 1])\n",
    "#         output = tf.image.resize_images(output, [16, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#         output = tf.transpose(output, [0, 3, 1, 2])\n",
    "#         output = Normalize('NoiseProvider.BN3', [0,2,3], output)\n",
    "\n",
    "#         output = lib.ops.conv2d.Conv2D('NoiseProvider.4', 2*dim, dim, 5, output, stride=1)\n",
    "#         output = tf.transpose(output, [0, 2, 3, 1])\n",
    "#         output = tf.image.resize_images(output, [32, 32], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#         output = tf.transpose(output, [0, 3, 1, 2])\n",
    "#         output = Normalize('NoiseProvider.BN4', [0,2,3], output)\n",
    "\n",
    "#         output = lib.ops.conv2d.Conv2D('NoiseProvider.5', dim, 3, 5, output, stride=1)\n",
    "#         output = tf.transpose(output, [0, 2, 3, 1])\n",
    "#         output = tf.image.resize_images(output, [64, 64], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#         output = tf.transpose(output, [0, 3, 1, 2])\n",
    "\n",
    "#         lib.ops.conv2d.unset_weights_stdev()\n",
    "#         lib.ops.deconv2d.unset_weights_stdev()\n",
    "#         lib.ops.linear.unset_weights_stdev()\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIM_G = 128 # Generator dimensionality\n",
    "DIM_D = 128 # Critic dimensionality\n",
    "NORMALIZATION_G = True # Use batchnorm in generator?\n",
    "NORMALIZATION_D = False # Use batchnorm (or layernorm) in critic?\n",
    "OUTPUT_DIM = 3072 # Number of pixels in CIFAR10 (32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UpsampleConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.concat([output, output, output, output], axis=1)\n",
    "    output = tf.transpose(output, [0,2,3,1])\n",
    "    output = tf.depth_to_space(output, 2)\n",
    "    output = tf.transpose(output, [0,3,1,2])\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases)\n",
    "    return output\n",
    "\n",
    "def ResidualBlock(name, input_dim, output_dim, filter_size, inputs, resample=None, no_dropout=False, labels=None):\n",
    "    \"\"\"\n",
    "    resample: None, 'down', or 'up'\n",
    "    \"\"\"\n",
    "    if resample=='down':\n",
    "        conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim, output_dim=input_dim)\n",
    "        conv_2        = functools.partial(ConvMeanPool, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_shortcut = ConvMeanPool\n",
    "    elif resample=='up':\n",
    "        conv_1        = functools.partial(UpsampleConv, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_shortcut = UpsampleConv\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=output_dim, output_dim=output_dim)\n",
    "    elif resample==None:\n",
    "        conv_shortcut = lib.ops.conv2d.Conv2D\n",
    "        conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=output_dim, output_dim=output_dim)\n",
    "    else:\n",
    "        raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim==input_dim and resample==None:\n",
    "        shortcut = inputs # Identity skip-connection\n",
    "    else:\n",
    "        shortcut = conv_shortcut(name+'.Shortcut', input_dim=input_dim, output_dim=output_dim, filter_size=1, he_init=False, biases=True, inputs=inputs)\n",
    "\n",
    "    output = inputs\n",
    "    output = Normalize(name+'.N1', output, labels=labels)\n",
    "    output = nonlinearity(output)\n",
    "    output = conv_1(name+'.Conv1', filter_size=filter_size, inputs=output)    \n",
    "    output = Normalize(name+'.N2', output, labels=labels)\n",
    "    output = nonlinearity(output)            \n",
    "    output = conv_2(name+'.Conv2', filter_size=filter_size, inputs=output)\n",
    "\n",
    "    return shortcut + output\n",
    "\n",
    "def OptimizedResBlockDisc1(inputs):\n",
    "    print(\"opt shape:\", inputs.shape)\n",
    "    conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=3, output_dim=DIM_D)\n",
    "    conv_2        = functools.partial(ConvMeanPool, input_dim=DIM_D, output_dim=DIM_D)\n",
    "    \n",
    "    conv_shortcut = MeanPoolConv\n",
    "    shortcut = conv_shortcut('Discriminator.1.Shortcut', input_dim=3, output_dim=DIM_D, filter_size=1, he_init=False, biases=True, inputs=inputs)\n",
    "    \n",
    "    output = inputs\n",
    "    output = conv_1('Discriminator.1.Conv1', filter_size=3, inputs=output)    \n",
    "    output = nonlinearity(output)            \n",
    "    output = conv_2('Discriminator.1.Conv2', filter_size=3, inputs=output)\n",
    "    return shortcut + output\n",
    "\n",
    "def nonlinearity(x):\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Generator(n_samples, labels, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random_normal([n_samples, 128])\n",
    "    with tf.variable_scope(\"S_images\", reuse = False):\n",
    "        output = lib.ops.linear.Linear('Generator.Input', 128, 4*4*DIM_G, noise)\n",
    "        output = tf.reshape(output, [-1, DIM_G, 4, 4])\n",
    "        output = ResidualBlock('Generator.1', DIM_G, DIM_G, 3, output, resample='up', labels=labels)\n",
    "        output = ResidualBlock('Generator.2', DIM_G, DIM_G, 3, output, resample='up', labels=labels)\n",
    "        output = ResidualBlock('Generator.3', DIM_G, DIM_G, 3, output, resample='up', labels=labels)\n",
    "        output = Normalize('Generator.OutputN', output)\n",
    "        output = nonlinearity(output)\n",
    "        output = lib.ops.conv2d.Conv2D('Generator.Output', DIM_G, 3, 3, output, he_init=False)\n",
    "        output = tf.tanh(output)\n",
    "        \n",
    "        return tf.transpose(tf.reshape(output, [-1, 3, 32, 32]), [0, 2, 3, 1])\n",
    "\n",
    "def GoodDiscriminator(inputs, dim=64, nonlinearity = swish, bn = True, reuse = False, labels = None):\n",
    "    print(\"Good discriminator shape:\", inputs.shape)\n",
    "    output = tf.reshape(tf.transpose(inputs, [0, 3, 1, 2]), [-1, 3, 32, 32])\n",
    "\n",
    "#     output = tf.reshape(inputs, [-1, 3, 32, 32])\n",
    "    with tf.variable_scope(\"layers\", reuse = reuse):\n",
    "        output = OptimizedResBlockDisc1(output)\n",
    "        output = ResidualBlock('Discriminator.2', DIM_D, DIM_D, 3, output, resample='down', labels=labels)\n",
    "        output = ResidualBlock('Discriminator.3', DIM_D, DIM_D, 3, output, resample=None, labels=labels)\n",
    "        output = ResidualBlock('Discriminator.4', DIM_D, DIM_D, 3, output, resample=None, labels=labels)\n",
    "        output = nonlinearity(output)\n",
    "        output = tf.reduce_mean(output, axis=[2,3])\n",
    "        output_wgan = lib.ops.linear.Linear('Discriminator.Output', DIM_D, 1, output)\n",
    "        output_wgan = tf.reshape(output_wgan, [-1])\n",
    "#         if CONDITIONAL and ACGAN:\n",
    "#             output_acgan = lib.ops.linear.Linear('Discriminator.ACGANOutput', DIM_D, 10, output)\n",
    "#             return output_wgan, output_acgan\n",
    "#         else:\n",
    "        return output_wgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "This subsection builds the WINN's discriminator and sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.879723Z",
     "start_time": "2017-08-24T23:09:24.805442Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(batch_shape, LAMBDA=10.0):\n",
    "    '''\n",
    "    Build a network for WINN.\n",
    "        batch_shape: Shape of a mini-batch in classification-step and synthesis-step.\n",
    "                     The format is [batch size, height, width, channels].\n",
    "        LAMBDA: the weight for the gradient penalty term\n",
    "    Return loss, trainable variables, labels and images in discriminator and \n",
    "    sampler, plus checkpoint saver. \n",
    "    '''\n",
    "\n",
    "    # Fetch batch shape.\n",
    "    [batch_size, height, width, channels] = batch_shape\n",
    "    \n",
    "    half_b_size = batch_size / 2\n",
    "    \n",
    "    # Placeholder for images and labels.\n",
    "    D_pos_images = tf.placeholder(dtype = tf.float32, \n",
    "                              shape = [half_b_size, height, width, channels], \n",
    "                              name = 'D_pos_images')\n",
    "    D_neg_images = tf.placeholder(dtype = tf.float32, \n",
    "                              shape = [half_b_size, height, width, channels], \n",
    "                              name = 'D_neg_images')\n",
    "    \n",
    "    # Variable, placeholder and assign operator for multiple sampled images.\n",
    "#     S_images = tf.Variable(\n",
    "#         # Use uniform distribution Unif(-1, 1) to initialize.\n",
    "#         # This initialization doesn't matter.\n",
    "#         # It will be substituted by S_images_op.\n",
    "#         np.random.uniform(low = -1.0,\n",
    "#                           high = 1.0, \n",
    "#                           size = [batch_size, height, width, channels]\n",
    "#         ).astype('float32'), \n",
    "#         name='S_images'\n",
    "#     )\n",
    "#     S_images_placeholder = tf.placeholder(dtype = S_images.dtype, \n",
    "#                                           shape = S_images.get_shape())\n",
    "#     S_images_op = S_images.assign(S_images_placeholder)\n",
    "\n",
    "    # Build a discriminator used in classification-step\n",
    "    D_pos_logits = GoodDiscriminator(D_pos_images, reuse = False)\n",
    "    D_neg_logits = GoodDiscriminator(D_neg_images, reuse = True)\n",
    "    D_loss = tf.reduce_mean(D_neg_logits - D_pos_logits)\n",
    "    D_pos_loss = tf.reduce_mean(D_pos_logits)\n",
    "    epsilon = tf.random_uniform([half_b_size, 1, 1, 1], 0.0, 1.0)\n",
    "    # Dirty hack to tile the tensor\n",
    "    epsilon = epsilon + tf.zeros(D_pos_images.shape, dtype=epsilon.dtype)\n",
    "    x_hat = epsilon * D_pos_images + (1 - epsilon) * D_neg_images\n",
    "    d_hat = GoodDiscriminator(x_hat, reuse = True)\n",
    "    \n",
    "    ddx = tf.gradients(d_hat, x_hat)[0]\n",
    "    ddx = tf.sqrt(tf.reduce_sum(tf.square(ddx), axis=[1, 2, 3]))\n",
    "    ddx = tf.reduce_mean(tf.square(ddx - 1.0) * LAMBDA)\n",
    "    D_loss += ddx\n",
    "    \n",
    "    # We need to store these values as they will be used for determining early-stopping threshold in testing stage\n",
    "    D_pos_loss_min = tf.Variable(0.0, name='D_pos_loss_min')\n",
    "    D_pos_loss_max = tf.Variable(0.0, name='D_pos_loss_max')\n",
    "    \n",
    "    D_pos_loss_min_placeholder = tf.placeholder(dtype = D_pos_loss_min.dtype, \n",
    "                                          shape = D_pos_loss_min.get_shape())\n",
    "    D_pos_loss_max_placeholder = tf.placeholder(dtype = D_pos_loss_max.dtype, \n",
    "                                          shape = D_pos_loss_max.get_shape())\n",
    "    D_pos_loss_min_op = D_pos_loss_min.assign(D_pos_loss_min_placeholder)\n",
    "    D_pos_loss_max_op = D_pos_loss_max.assign(D_pos_loss_max_placeholder)\n",
    "    \n",
    "    # Build a sampler used in synthesis-step\n",
    "    S_images = Generator(batch_size, labels= None)\n",
    "    S_logits = GoodDiscriminator(S_images, reuse = True)\n",
    "    S_loss = tf.reduce_mean(S_logits)\n",
    "\n",
    "    # Variable, placeholder and assign operator for multiple generated images.\n",
    "#     small_noise = tf.Variable(\n",
    "#         np.random.uniform(low = -1.0,\n",
    "#                           high = 1.0, \n",
    "#                           size = [batch_size, 512, 4, 4]\n",
    "#         ).astype('float32'),\n",
    "#         name='small_noise'\n",
    "#     )\n",
    "#     small_noise_placeholder = tf.placeholder(dtype = small_noise.dtype, \n",
    "#                                           shape = small_noise.get_shape())\n",
    "#     small_noise_op = small_noise.assign(small_noise_placeholder)\n",
    "    \n",
    "    big_noise = Generator(100, labels= None)\n",
    "    # Variables to train.\n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    D_vars = [var for var in trainable_vars if 'layers' in var.name]\n",
    "    S_vars = [var for var in trainable_vars if 'S_images' in var.name]\n",
    "    \n",
    "    # Checkpoint saver.\n",
    "    saver = tf.train.Saver(max_to_keep = 5000)\n",
    "    \n",
    "    return [D_loss, S_loss, D_vars, S_vars, \n",
    "            D_pos_images, D_neg_images, S_images,\n",
    "            saver, D_pos_loss, D_pos_loss_min, D_pos_loss_max,\n",
    "            D_pos_loss_min_placeholder, D_pos_loss_max_placeholder,\n",
    "            D_pos_loss_min_op, D_pos_loss_max_op, \n",
    "            big_noise]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This section focuses on model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From https://github.com/Mazecreator/tensorflow-hints/tree/master/maximize\n",
    "def maximize(optimizer, loss, **kwargs):\n",
    "      return optimizer.minimize(-loss, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T23:09:24.909398Z",
     "start_time": "2017-08-24T23:09:24.881376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimizers(D_loss, S_loss, D_vars, S_vars):\n",
    "    '''\n",
    "    Get optimizers.\n",
    "        D_loss: Discriminator loss.\n",
    "        S_loss: Sampler loss.\n",
    "        D_vars: Variables to train in discriminator.\n",
    "        S_vars: Variable to train in sampler = image.\n",
    "    Return optimizer of discriminator and sampler, plus discriminator \n",
    "    learning rate, discriminator global steps and the initializer for sampler.\n",
    "    '''\n",
    "    \n",
    "    # Scope of discriminator optimizer.\n",
    "    with tf.variable_scope('D_optimizer'):\n",
    "        # Global count of step in discriminator and increment operator.\n",
    "        # It should not be trainable and should be adjusted by training process.\n",
    "        D_global_step = tf.Variable(initial_value = 0, trainable = False)\n",
    "        D_global_step_op = D_global_step.assign_add(1)\n",
    "        # Learning rate with exponential decay.\n",
    "        D_learning_rate = tf.train.exponential_decay(learning_rate = 0.0001,\n",
    "                                                     global_step = D_global_step,\n",
    "                                                     decay_steps = 100,\n",
    "                                                     decay_rate = 0.9,\n",
    "                                                     staircase = True)        \n",
    "        D_adam = tf.train.AdamOptimizer(learning_rate = D_learning_rate, beta1=0., beta2=0.9)\n",
    "        D_optimizer = D_adam.minimize(loss = D_loss, var_list = D_vars)\n",
    "        \n",
    "    # Scope of sampler optimizer.\n",
    "    with tf.variable_scope('S_optimizer'):\n",
    "        S_global_step = tf.Variable(initial_value = 0, trainable = False, name = 'S_step')\n",
    "        S_learning_rate = 0.0001\n",
    "\n",
    "        S_adam = tf.train.AdamOptimizer(learning_rate = S_learning_rate, beta1 = 0., beta2=0.)\n",
    "        S_optimizer = maximize(optimizer = S_adam, loss = S_loss, var_list = S_vars)\n",
    "        \n",
    "    # Variables of sampler optimizer and initializer operator of that.\n",
    "    S_optimizer_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \n",
    "                                         scope = 'WINN/S_optimizer')\n",
    "    print (\"S_optimizer_vars\", S_optimizer_vars)\n",
    "    S_initializer_op = tf.variables_initializer(S_optimizer_vars)\n",
    "\n",
    "    # Variables of sampler optimizer and initializer operator of that.\n",
    "    print(S_optimizer_vars)\n",
    "\n",
    "    return [D_optimizer, S_optimizer, D_learning_rate, S_initializer_op, S_global_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-24T23:09:40.350Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train(sess):\n",
    "#     \"\"\"\n",
    "#     Train the WINN model.\n",
    "#     sess: Session.\n",
    "#     \"\"\"\n",
    "#     file_name = \"model-0.0001_lr-one-batch-gen-disc-improv\"\n",
    "#     # Set timer.\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     half_batch_size = batch_size // 2\n",
    "#     sqrt_batch_size = int(np.sqrt(batch_size))\n",
    "\n",
    "#     # Log file path.\n",
    "#     log_file_path = os.path.join(data_dir_path, \"log\" +file_name+\".txt\")\n",
    "#     # Prepare for root directory of model.\n",
    "#     model_root = os.path.join(data_dir_path, file_name)\n",
    "#     mkdir_if_not_exists(model_root)\n",
    "#     # Prepare for root directory of intermediate image.\n",
    "#     intermediate_image_root = os.path.join(data_dir_path, \"intermediate\"+file_name)\n",
    "#     mkdir_if_not_exists(intermediate_image_root)\n",
    "#     # Prepare for root directory of negative images.\n",
    "#     neg_image_root = os.path.join(data_dir_path, \"negative\"+file_name)\n",
    "#     mkdir_if_not_exists(neg_image_root)\n",
    "        \n",
    "#     ######################################################################\n",
    "#     # Training stage 1: Load positive images.\n",
    "#     ######################################################################\n",
    "#     log(log_file_path,\n",
    "#         \"Training stage 1: Load positive images...\")\n",
    "\n",
    "#     # Path of all positive images and negative images. \n",
    "#     # The following training_images_dir_path can be replaced. The image shape of\n",
    "#     # positive and negative images are the same.\n",
    "    \n",
    "#     pos_all_images_path = get_images_path_in_directory(training_images_dir_path)\n",
    "#     image_shape = get_image_shape(pos_all_images_path[0])\n",
    "\n",
    "#     ######################################################################\n",
    "#     # Training stage 2: Build network and initialize.\n",
    "#     ######################################################################\n",
    "#     log(log_file_path,\n",
    "#         \"Training stage 2: Build network and initialize...\")\n",
    "#     height, width, channels = image_shape\n",
    "#     print(image_shape)\n",
    "    \n",
    "#     # Build network.\n",
    "#     [D_loss, S_loss, D_vars, S_vars, \n",
    "#      D_pos_images, D_neg_images, S_images, \n",
    "#      saver, D_pos_loss, D_pos_loss_min, D_pos_loss_max,\n",
    "#      D_pos_loss_min_placeholder, D_pos_loss_max_placeholder,\n",
    "#      D_pos_loss_min_op, D_pos_loss_max_op, \n",
    "#      big_noise] = \\\n",
    "#         build_network(batch_shape = [batch_size, height, width, channels])\n",
    "        \n",
    "#     # Get optimizer.\n",
    "#     [D_optimizer, S_optimizer, D_learning_rate, S_initializer_op, S_global_step] = \\\n",
    "#         get_optimizers(D_loss = D_loss, S_loss = S_loss, \n",
    "#                        D_vars = D_vars, S_vars = S_vars)\n",
    "        \n",
    "#     # Show a list of global variables.\n",
    "#     global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')\n",
    "#     log(log_file_path, \"Global variables:\")\n",
    "#     for i, var in enumerate(global_variables):\n",
    "#         log(log_file_path, \"{0} {1}\".format(i, var.name))\n",
    "        \n",
    "#     # Initialize all variables.\n",
    "#     all_initializer_op = tf.global_variables_initializer()\n",
    "#     sess.run(all_initializer_op)\n",
    "    \n",
    "#     # Generate initial pseudo-negative images\n",
    "#     # In fact, the name of image has format \n",
    "#     #     {cascade}_{next iteration}_{i}.png\n",
    "#     # where cascade means current cascade model, next iteration means\n",
    "#     # next iteration of sampler and discriminator training, and i means\n",
    "#     # the index of images.\n",
    "#     neg_image_path = os.path.join(neg_image_root, 'cascade_{0}_iteration_{1}_count_{2}.png')\n",
    "#     neg_init_images_count = 10000\n",
    "#     neg_init_images_path = [neg_image_path.format(0, 0, i) \\\n",
    "#                             for i in range(neg_init_images_count)]\n",
    "    \n",
    "#     S_iteration_count_of_batch = neg_init_images_count // batch_size\n",
    "#     print(\"S_iteration_count_of_batch:\", S_iteration_count_of_batch)\n",
    "    \n",
    "#     for i in xrange(S_iteration_count_of_batch):             \n",
    "# #         small_noise_batch = np.random.uniform(low=-1.0, high=1.0, size=(100, 512, 4, 4))\n",
    "# #         sess.run(small_noise_op, {small_noise_placeholder: \n",
    "# #                        small_noise_batch})\n",
    "#         np_noise_images = np.transpose(sess.run(big_noise), axes=[0, 2, 3, 1])\n",
    "\n",
    "#         # Generate random images as negatives and save them.\n",
    "#         for j in range(100):#, neg_init_image_path in enumerate(neg_init_images_path):\n",
    "#             # Attention: Though it is called neg_image here, it has 4 dimensions,\n",
    "#             #            that is, [1, height, width, channels], which is not a\n",
    "#             #            pure single image, which is [height, width, channels].\n",
    "#             #            So we still use save_unnormalized_images here instead of \n",
    "#             #            save_unnormalized_image.\n",
    "#             neg_image = np_noise_images[j].reshape(1, 32, 32, 3)\n",
    "#             neg_image = neg_image - neg_image.min()\n",
    "#             neg_image = neg_image / neg_image.max() * 255.0 \n",
    "#             save_unnormalized_images(images = neg_image, \n",
    "#                                      size = (1, 1), path = neg_init_images_path[batch_size * i + j])\n",
    "\n",
    "#     neg_all_images_path = neg_init_images_path\n",
    "    \n",
    "#     ######################################################\n",
    "#     log(log_file_path,\n",
    "#         \"Positive images {0}, negative images {1}, image shape {2}\".format(\n",
    "#         len(pos_all_images_path), len(neg_all_images_path), image_shape))\n",
    "    \n",
    "#     ######################################################################\n",
    "#     # Training stage 3: Cascades training.\n",
    "#     ######################################################################\n",
    "#     log(log_file_path, \"Training stage 3: Cascades training...\")\n",
    "        \n",
    "#     # One cascade means one new model.\n",
    "#     # p_{W^n}^- <- [cascade n] <- [cascade n-1] <- ... <- [cascade 0] <- p_r \n",
    "#     # where p_r means Uniform or Gaussian reference distribution \n",
    "#     # and p_{W^n}^- means distribution of pseudo-negatives after n cascades.\n",
    "#     # One cascade training consists multiple iterations (by default 100 iterations).\n",
    "    \n",
    "#     # Prepare for the initial images to feed the sampler. In fact, it is \n",
    "#     # because we always use negative images in last cascade as the \"initial\"\n",
    "#     # images to feed sampler in all iterations of current cascade.\n",
    "#     S_neg_last_cascade_images_path = copy.deepcopy(neg_all_images_path)\n",
    "    \n",
    "#     for cascade in xrange(cascades):\n",
    "#         ######################################################################\n",
    "#         # Training stage 3.1: Iterations training.\n",
    "#         ######################################################################\n",
    "#         # One iteration means one time of discriminator training and one time\n",
    "#         # of sampling pseudo-negatives. One iteration training may contain multiple\n",
    "#         # batches for discriminator training and sampling pseudo-negatives.\n",
    "#         for iteration in xrange(iterations_per_cascade):\n",
    "#             ######################################################################\n",
    "#             # Training stage 3.1.1: Prepare images and labels for discriminator\n",
    "#             # training.\n",
    "#             ######################################################################\n",
    "#             # Count of positive images to train in current iteration.\n",
    "#             D_pos_iteration_images_count = min(iteration + 1, 5) * 100 \\\n",
    "#                 // half_batch_size * half_batch_size\n",
    "            \n",
    "#             if D_pos_iteration_images_count >= len(pos_all_images_path):\n",
    "#                 # When the number of all positive images is more than current\n",
    "#                 # iteration negative images, we allow duplicate images.\n",
    "#                 D_pos_iteration_images_path = np.random.choice(\n",
    "#                     pos_all_images_path, \n",
    "#                     size = D_pos_iteration_images_count, \n",
    "#                     replace = True\n",
    "#                 ).tolist()\n",
    "#             else:\n",
    "#                 # When the number of all positive images is less or equal than \n",
    "#                 # current iteration negative images, we require unique images.\n",
    "#                 D_pos_iteration_images_path = np.random.choice(\n",
    "#                     pos_all_images_path, \n",
    "#                     size = D_pos_iteration_images_count, \n",
    "#                     replace = False\n",
    "#                 ).tolist()\n",
    "\n",
    "#             # Here we consider the \"save all\" mode in Long Jin's code. This mode\n",
    "#             # has different behaviors on discriminator and sampler.\n",
    "#             # 1) Discriminator.\n",
    "#             #     We draw positive images from training dataset and the same\n",
    "#             #     number of negative images from *all* pseudo-negative images in data/negative folder.\n",
    "#             #     Every iteration of sampler will add newly generated negative images\n",
    "#             #     into all data/negative foler.\n",
    "#             # 2) Sampler.\n",
    "#             #     We draw \"initial\" negative images in every iteration in current\n",
    "#             #     cascade from part of generated negative images in last cascade.\n",
    "#             #     More specificially, the part is the *last* iteration of last cascade.\n",
    "#             D_neg_iteration_images_count = D_pos_iteration_images_count\n",
    "#             D_neg_iteration_images_path = np.random.choice(\n",
    "#                 neg_all_images_path,\n",
    "#                 D_pos_iteration_images_count, \n",
    "#                 replace = True).tolist()\n",
    "                            \n",
    "#             log(log_file_path,\n",
    "#                    (\"Discriminator: Cascade {0}, iteration {1}, \" + \n",
    "#                    \"all pos {2}, all neg {3}, \" + \n",
    "#                    \"current iteration {4} (pos {5}, neg {6}), \" + \n",
    "#                    \"learning rate {7}\").format(\n",
    "#                        cascade, iteration, \n",
    "#                        len(pos_all_images_path), len(neg_all_images_path), \n",
    "#                        D_pos_iteration_images_count + D_neg_iteration_images_count, \n",
    "#                        D_pos_iteration_images_count, D_neg_iteration_images_count, \n",
    "#                        sess.run(D_learning_rate)\n",
    "#                    ))\n",
    "            \n",
    "#             ######################################################################\n",
    "#             # Training stage 3.1.2: Train the discriminator.\n",
    "#             ######################################################################\n",
    "#             # Count of batch in discriminator training in current iteration. \n",
    "#             D_iteration_count_of_batch = len(D_pos_iteration_images_path) // half_batch_size\n",
    "            \n",
    "#             min_D_batch_pos_loss = inf\n",
    "#             max_D_batch_pos_loss = -inf\n",
    "#             print(\"D_iteration_count_of_batch:\", D_iteration_count_of_batch)\n",
    "#             for nc in range(k):\n",
    "#                 for i in xrange(D_iteration_count_of_batch):\n",
    "#                     # Load images for this batch in discriminator.\n",
    "#                     D_pos_batch_images = [load_unnormalized_image(path) for path in\n",
    "#                         D_pos_iteration_images_path[i * half_batch_size : (i + 1) * half_batch_size]]\n",
    "#                     D_neg_batch_images = [load_unnormalized_image(path) for path in\n",
    "#                         D_neg_iteration_images_path[i * half_batch_size : (i + 1) * half_batch_size]]\n",
    "#                     # Normalize.\n",
    "#                     D_pos_batch_images = normalize(np.array(D_pos_batch_images)).astype(np.float32)\n",
    "#                     D_neg_batch_images = normalize(np.array(D_neg_batch_images)).astype(np.float32)\n",
    "\n",
    "#                     sess.run(D_optimizer, \n",
    "#                              feed_dict = {D_pos_images: D_pos_batch_images,\n",
    "#                                           D_neg_images: D_neg_batch_images})\n",
    "#                     # Optmize both the discriminator together one after another\n",
    "#                     # change **************\n",
    "#                     # Optimize.\n",
    "#                     S_neg_last_cascade_images_path = shuffle(S_neg_last_cascade_images_path)\n",
    "#                     # Attention again, the last cascade here does not mean all negative images\n",
    "#                     # produced in last cascade, but only negative images in last iteration of\n",
    "#                     # last cascade.\n",
    "\n",
    "#                     # Number of negative images to be generated in current iteration.\n",
    "#                     if iteration == iterations_per_cascade - 1:\n",
    "#                         # Generate more in last iteration of cascade.\n",
    "#                         S_neg_iteration_images_count = 10000\n",
    "#                     else:\n",
    "#                         S_neg_iteration_images_count = 1000\n",
    "\n",
    "#                     S_neg_current_iteration_images_path = \\\n",
    "#                         [os.path.join(neg_image_root, 'cascade_{0}_iteration_{1}_count_{2}.png').format(\n",
    "#                             cascade, iteration + 1, i) for i in xrange(\n",
    "#                                 S_neg_iteration_images_count)]\n",
    "\n",
    "#                     log(log_file_path,\n",
    "#                           (\"Sampler: Cascade {0}, iteration {1}, \" + \n",
    "#                            \"current iteration neg {2}\").format(\n",
    "#                            cascade, iteration, \n",
    "#                            S_neg_iteration_images_count))\n",
    "                    \n",
    "#                     sess.run(S_optimizer)\n",
    "#                     S_neg_intermediate_images = sess.run(S_images)\n",
    "#                     [_, height, width, channels] = S_neg_intermediate_images.shape\n",
    "#                     for j in xrange(batch_size):\n",
    "#                         save_unnormalized_image(\n",
    "#                             image = unnormalize(S_neg_intermediate_images[j,:,:,:]),  \n",
    "#                             path = S_neg_current_iteration_images_path[iteration * batch_size + j])\n",
    "\n",
    "#                     # Output information every 100 batches.\n",
    "#                     if i % 100 == 0:\n",
    "#                         log(log_file_path,\n",
    "#                               (\"Sampler: Cascade {0}, iteration {1}, batch {2}, \" + \n",
    "#                                \"time {3}, S_loss {4}\").format(\n",
    "#                                cascade, iteration, i, \n",
    "#                                time.time() - start_time, sess.run(S_loss)))\n",
    "\n",
    "#                     # Clip and re-feed to sampler.\n",
    "#     #                     sess.run(S_images_op, feed_dict = {S_images_placeholder: \n",
    "#     #                                                        np.clip(sess.run(S_images), -1.0, 1.0)})\n",
    "#                     # Stop based on threshold.\n",
    "#                     # The threshold is based on real samples' score.\n",
    "#                     # Update until the WINN network thinks pseudo-negative samples are quite close to real.\n",
    "# #                     if sess.run(S_loss) >= thres_:\n",
    "# #                         continue\n",
    "                    \n",
    "#                     # ***************    \n",
    "#                     if (nc == k - 1):\n",
    "#                         # Positive samples' loss after training in current iteration.\n",
    "#                         # It will be used as an early stopping threshold when we generate pseudo-negative samples\n",
    "#                         D_batch_pos_loss = sess.run(D_pos_loss, \n",
    "#                          feed_dict = {D_pos_images: D_pos_batch_images})\n",
    "#                         if (D_batch_pos_loss < min_D_batch_pos_loss):\n",
    "#                             min_D_batch_pos_loss = D_batch_pos_loss\n",
    "#                         if (D_batch_pos_loss > max_D_batch_pos_loss):\n",
    "#                             max_D_batch_pos_loss = D_batch_pos_loss\n",
    "#                 # Discriminator loss after training in current iteration.\n",
    "#                 D_last_batch_loss = sess.run(D_loss, \n",
    "#                      feed_dict = {D_pos_images: D_pos_batch_images,\n",
    "#                                   D_neg_images: D_neg_batch_images})\n",
    "                \n",
    "#                 log(log_file_path, \n",
    "#                     \"Discriminator: Cascade {0}, iteration {1}, Critic {2}, time {3}, D_loss {4}, D_pos_loss {5}, {6}\".format(\n",
    "#                     cascade, iteration, nc, time.time() - start_time, D_last_batch_loss, min_D_batch_pos_loss, max_D_batch_pos_loss))\n",
    "        \n",
    "#             # Save last batch images in discriminator training.\n",
    "#             D_intermediate_image_path = os.path.join(intermediate_image_root,\n",
    "#                 'D_cascade_{0}_iteration_{1}.png').format(cascade, iteration)\n",
    "#             save_unnormalized_images(images = unnormalize(np.concatenate((D_pos_batch_images, \\\n",
    "#                                                                           D_neg_batch_images), axis=0)), \n",
    "#                                      size = (sqrt_batch_size, sqrt_batch_size), \n",
    "#                                      path = D_intermediate_image_path)\n",
    "            \n",
    "            \n",
    "#             # After current iteration, new negative images will be added into the set of\n",
    "#             # negative images. Note that we keep all previous pseudo-negative images\n",
    "#             # to prevent the classifier forgetting what it has learned in previous stages\n",
    "\n",
    "#             neg_all_images_path += S_neg_current_iteration_images_path\n",
    "#             ######################################################################\n",
    "#             # Training stage 3.1.3: Initialize pseudo-negatives.\n",
    "#             ######################################################################\n",
    "# #\n",
    "#             # Load path of negative images in last cascade and shuffle.\n",
    "# #             S_neg_last_cascade_images_path = shuffle(S_neg_last_cascade_images_path)\n",
    "# #             # Attention again, the last cascade here does not mean all negative images\n",
    "# #             # produced in last cascade, but only negative images in last iteration of\n",
    "# #             # last cascade.\n",
    "\n",
    "# #             # Number of negative images to be generated in current iteration.\n",
    "# #             if iteration == iterations_per_cascade - 1:\n",
    "# #                 # Generate more in last iteration of cascade.\n",
    "# #                 S_neg_iteration_images_count = 10000\n",
    "# #             else:\n",
    "# #                 S_neg_iteration_images_count = 1000\n",
    "            \n",
    "# #             S_neg_current_iteration_images_path = \\\n",
    "# #                 [os.path.join(neg_image_root, 'cascade_{0}_iteration_{1}_count_{2}.png').format(\n",
    "# #                     cascade, iteration + 1, i) for i in xrange(\n",
    "# #                         S_neg_iteration_images_count)]\n",
    "            \n",
    "# #             log(log_file_path,\n",
    "# #                   (\"Sampler: Cascade {0}, iteration {1}, \" + \n",
    "# #                    \"current iteration neg {2}\").format(\n",
    "# #                    cascade, iteration, \n",
    "# #                    S_neg_iteration_images_count))\n",
    "                  \n",
    "#             # Save early-stopping threshold in the model\n",
    "#             if iteration == iterations_per_cascade - 1:\n",
    "#                 sess.run(D_pos_loss_min_op, {D_pos_loss_min_placeholder: \n",
    "#                        min_D_batch_pos_loss})\n",
    "#                 sess.run(D_pos_loss_max_op, {D_pos_loss_max_placeholder: \n",
    "#                        max_D_batch_pos_loss})\n",
    "\n",
    "#             ######################################################################\n",
    "#             # Training stage 3.1.3: Sample pseudo-negatives.\n",
    "#             ######################################################################\n",
    "#             # Count of batch  in current iteration. \n",
    "#             S_iteration_count_of_batch = S_neg_iteration_images_count // batch_size\n",
    "            \n",
    "#             # Initializer the image in sampler. However, it is strange because\n",
    "#             # we will feed the S_images later. It is only needed if we want to\n",
    "#             # generate images from noise. So we ignore it at first.\n",
    "#             sess.run(S_initializer_op)\n",
    "#             sess.run(S_global_step.initializer)\n",
    "\n",
    "\n",
    "#             # Generating process. We may optimize images for several times\n",
    "#             # to get good images. Early stopping is used here to accelerate.\n",
    "#             thres_ = np.random.uniform(min_D_batch_pos_loss, max_D_batch_pos_loss)\n",
    "# #             count_of_optimizing_steps = 2000\n",
    "#             print(\"S_iteration_count_of_batch:\", S_iteration_count_of_batch)\n",
    "#             #********** change\n",
    "# #             for j in range(count_of_optimizing_steps):\n",
    "# #                 # Optimize.\n",
    "# #                 sess.run(S_optimizer)\n",
    "\n",
    "# #                 # Stop based on threshold.\n",
    "# #                 # The threshold is based on real samples' score.\n",
    "# #                 # Update until the WINN network thinks pseudo-negative samples are quite close to real.\n",
    "# #                 if sess.run(S_loss) >= thres_:\n",
    "# #                     break\n",
    "#             #***************\n",
    "# #             for i in xrange(S_iteration_count_of_batch):\n",
    "#                 # Save intermediate negative images in sampler.\n",
    "# #                 S_neg_intermediate_images = sess.run(S_images)\n",
    "# #                 [_, height, width, channels] = S_neg_intermediate_images.shape\n",
    "# #                 for j in xrange(batch_size):\n",
    "# #                     save_unnormalized_image(\n",
    "# #                         image = unnormalize(S_neg_intermediate_images[j,:,:,:]),  \n",
    "# #                         path = S_neg_current_iteration_images_path[i * batch_size + j])\n",
    "\n",
    "# #                 # Output information every 100 batches.\n",
    "# #                 if i % 100 == 0:\n",
    "# #                     log(log_file_path,\n",
    "# #                           (\"Sampler: Cascade {0}, iteration {1}, batch {2}, \" + \n",
    "# #                            \"time {3}, S_loss {4}\").format(\n",
    "# #                            cascade, iteration, i, \n",
    "# #                            time.time() - start_time, sess.run(S_loss)))\n",
    "\n",
    "# #             # After current iteration, new negative images will be added into the set of\n",
    "# #             # negative images. Note that we keep all previous pseudo-negative images\n",
    "# #             # to prevent the classifier forgetting what it has learned in previous stages\n",
    "            \n",
    "# #             neg_all_images_path += S_neg_current_iteration_images_path\n",
    "# #************\n",
    "#             # Save last batch images in sampling pseudo-negatives stage.\n",
    "#             S_neg_intermediate_image_path = os.path.join(intermediate_image_root,\n",
    "#                 'S_cascade_{0}_iteration_{1}.png').format(cascade, iteration)\n",
    "#             # In discriminator we save D_batch_images, but here we use \n",
    "#             # S_intermediate_images. It is because we always use *_batch_images\n",
    "#             # to represent the images we put in the discriminator or sampler.\n",
    "#             # So G_neg_batch_images should be the \"initial\" images in current \n",
    "#             # iteration and S_neg_intermediate_images is the generated images.\n",
    "#             print(\"shape of neg images:\", S_neg_intermediate_images.shape)\n",
    "#             save_unnormalized_images(images = unnormalize(S_neg_intermediate_images), \n",
    "#                                      size = (sqrt_batch_size, sqrt_batch_size), \n",
    "#                                      path = S_neg_intermediate_image_path)\n",
    "            \n",
    "#         # Last cascade's generated negative images. More specifically, we only use\n",
    "#         # those images generated by last iteration of last cascade.\n",
    "#         S_neg_last_cascade_images_path = copy.deepcopy(S_neg_current_iteration_images_path)\n",
    "        \n",
    "#         # Save the model.\n",
    "#         saver.save(sess, (os.path.join(model_root, 'cascade-{}.model').format(cascade)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-24T23:09:40.362Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage 1: Load positive images...\n",
      "Training stage 2: Build network and initialize...\n",
      "[32, 32, 3]\n",
      "('Good discriminator shape:', TensorShape([Dimension(50), Dimension(32), Dimension(32), Dimension(3)]))\n",
      "('opt shape:', TensorShape([Dimension(50), Dimension(3), Dimension(32), Dimension(32)]))\n",
      "('Good discriminator shape:', TensorShape([Dimension(50), Dimension(32), Dimension(32), Dimension(3)]))\n",
      "('opt shape:', TensorShape([Dimension(50), Dimension(3), Dimension(32), Dimension(32)]))\n",
      "('Good discriminator shape:', TensorShape([Dimension(50), Dimension(32), Dimension(32), Dimension(3)]))\n",
      "('opt shape:', TensorShape([Dimension(50), Dimension(3), Dimension(32), Dimension(32)]))\n",
      "('Good discriminator shape:', TensorShape([Dimension(100), Dimension(32), Dimension(32), Dimension(3)]))\n",
      "('opt shape:', TensorShape([Dimension(100), Dimension(3), Dimension(32), Dimension(32)]))\n",
      "('S_optimizer_vars', [<tf.Variable 'WINN/S_optimizer/S_step:0' shape=() dtype=int32_ref>, <tf.Variable 'WINN/S_optimizer/beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.W/Adam:0' shape=(128, 2048) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.W/Adam_1:0' shape=(128, 2048) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.b/Adam:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.b/Adam_1:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters/Adam:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters/Adam_1:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters/Adam:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters/Adam_1:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters/Adam:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters/Adam_1:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Filters/Adam:0' shape=(3, 3, 128, 3) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Filters/Adam_1:0' shape=(3, 3, 128, 3) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Biases/Adam:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Biases/Adam_1:0' shape=(3,) dtype=float32_ref>])\n",
      "[<tf.Variable 'WINN/S_optimizer/S_step:0' shape=() dtype=int32_ref>, <tf.Variable 'WINN/S_optimizer/beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.W/Adam:0' shape=(128, 2048) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.W/Adam_1:0' shape=(128, 2048) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.b/Adam:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.b/Adam_1:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters/Adam:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters/Adam_1:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N1.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.N2.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters/Adam:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters/Adam_1:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N1.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.N2.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters/Adam:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters/Adam_1:0' shape=(1, 1, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N1.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.N2.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters/Adam:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters/Adam_1:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.offset/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.offset/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.scale/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.OutputN.scale/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Filters/Adam:0' shape=(3, 3, 128, 3) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Filters/Adam_1:0' shape=(3, 3, 128, 3) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Biases/Adam:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Biases/Adam_1:0' shape=(3,) dtype=float32_ref>]\n",
      "Global variables:\n",
      "0 WINN/layers/Discriminator.1.Shortcut/Discriminator.1.Shortcut.Filters:0\n",
      "1 WINN/layers/Discriminator.1.Shortcut/Discriminator.1.Shortcut.Biases:0\n",
      "2 WINN/layers/Discriminator.1.Conv1/Discriminator.1.Conv1.Filters:0\n",
      "3 WINN/layers/Discriminator.1.Conv1/Discriminator.1.Conv1.Biases:0\n",
      "4 WINN/layers/Discriminator.1.Conv2/Discriminator.1.Conv2.Filters:0\n",
      "5 WINN/layers/Discriminator.1.Conv2/Discriminator.1.Conv2.Biases:0\n",
      "6 WINN/layers/Discriminator.2.Shortcut/Discriminator.2.Shortcut.Filters:0\n",
      "7 WINN/layers/Discriminator.2.Shortcut/Discriminator.2.Shortcut.Biases:0\n",
      "8 WINN/layers/Discriminator.2.N1.offset:0\n",
      "9 WINN/layers/Discriminator.2.N1.scale:0\n",
      "10 WINN/layers/Discriminator.2.Conv1/Discriminator.2.Conv1.Filters:0\n",
      "11 WINN/layers/Discriminator.2.Conv1/Discriminator.2.Conv1.Biases:0\n",
      "12 WINN/layers/Discriminator.2.N2.offset:0\n",
      "13 WINN/layers/Discriminator.2.N2.scale:0\n",
      "14 WINN/layers/Discriminator.2.Conv2/Discriminator.2.Conv2.Filters:0\n",
      "15 WINN/layers/Discriminator.2.Conv2/Discriminator.2.Conv2.Biases:0\n",
      "16 WINN/layers/Discriminator.3.N1.offset:0\n",
      "17 WINN/layers/Discriminator.3.N1.scale:0\n",
      "18 WINN/layers/Discriminator.3.Conv1/Discriminator.3.Conv1.Filters:0\n",
      "19 WINN/layers/Discriminator.3.Conv1/Discriminator.3.Conv1.Biases:0\n",
      "20 WINN/layers/Discriminator.3.N2.offset:0\n",
      "21 WINN/layers/Discriminator.3.N2.scale:0\n",
      "22 WINN/layers/Discriminator.3.Conv2/Discriminator.3.Conv2.Filters:0\n",
      "23 WINN/layers/Discriminator.3.Conv2/Discriminator.3.Conv2.Biases:0\n",
      "24 WINN/layers/Discriminator.4.N1.offset:0\n",
      "25 WINN/layers/Discriminator.4.N1.scale:0\n",
      "26 WINN/layers/Discriminator.4.Conv1/Discriminator.4.Conv1.Filters:0\n",
      "27 WINN/layers/Discriminator.4.Conv1/Discriminator.4.Conv1.Biases:0\n",
      "28 WINN/layers/Discriminator.4.N2.offset:0\n",
      "29 WINN/layers/Discriminator.4.N2.scale:0\n",
      "30 WINN/layers/Discriminator.4.Conv2/Discriminator.4.Conv2.Filters:0\n",
      "31 WINN/layers/Discriminator.4.Conv2/Discriminator.4.Conv2.Biases:0\n",
      "32 WINN/layers/Discriminator.Output/Discriminator.Output.W:0\n",
      "33 WINN/layers/Discriminator.Output/Discriminator.Output.b:0\n",
      "34 WINN/D_pos_loss_min:0\n",
      "35 WINN/D_pos_loss_max:0\n",
      "36 WINN/S_images/Generator.Input/Generator.Input.W:0\n",
      "37 WINN/S_images/Generator.Input/Generator.Input.b:0\n",
      "38 WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters:0\n",
      "39 WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases:0\n",
      "40 WINN/S_images/Generator.1.N1.offset:0\n",
      "41 WINN/S_images/Generator.1.N1.scale:0\n",
      "42 WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters:0\n",
      "43 WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases:0\n",
      "44 WINN/S_images/Generator.1.N2.offset:0\n",
      "45 WINN/S_images/Generator.1.N2.scale:0\n",
      "46 WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters:0\n",
      "47 WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases:0\n",
      "48 WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters:0\n",
      "49 WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases:0\n",
      "50 WINN/S_images/Generator.2.N1.offset:0\n",
      "51 WINN/S_images/Generator.2.N1.scale:0\n",
      "52 WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters:0\n",
      "53 WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases:0\n",
      "54 WINN/S_images/Generator.2.N2.offset:0\n",
      "55 WINN/S_images/Generator.2.N2.scale:0\n",
      "56 WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters:0\n",
      "57 WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases:0\n",
      "58 WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters:0\n",
      "59 WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases:0\n",
      "60 WINN/S_images/Generator.3.N1.offset:0\n",
      "61 WINN/S_images/Generator.3.N1.scale:0\n",
      "62 WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters:0\n",
      "63 WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases:0\n",
      "64 WINN/S_images/Generator.3.N2.offset:0\n",
      "65 WINN/S_images/Generator.3.N2.scale:0\n",
      "66 WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters:0\n",
      "67 WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases:0\n",
      "68 WINN/S_images/Generator.OutputN.offset:0\n",
      "69 WINN/S_images/Generator.OutputN.scale:0\n",
      "70 WINN/S_images/Generator.Output/Generator.Output.Filters:0\n",
      "71 WINN/S_images/Generator.Output/Generator.Output.Biases:0\n",
      "72 WINN/D_optimizer/Variable:0\n",
      "73 WINN/D_optimizer/beta1_power:0\n",
      "74 WINN/D_optimizer/beta2_power:0\n",
      "75 WINN/D_optimizer/WINN/layers/Discriminator.1.Shortcut/Discriminator.1.Shortcut.Filters/Adam:0\n",
      "76 WINN/D_optimizer/WINN/layers/Discriminator.1.Shortcut/Discriminator.1.Shortcut.Filters/Adam_1:0\n",
      "77 WINN/D_optimizer/WINN/layers/Discriminator.1.Shortcut/Discriminator.1.Shortcut.Biases/Adam:0\n",
      "78 WINN/D_optimizer/WINN/layers/Discriminator.1.Shortcut/Discriminator.1.Shortcut.Biases/Adam_1:0\n",
      "79 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv1/Discriminator.1.Conv1.Filters/Adam:0\n",
      "80 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv1/Discriminator.1.Conv1.Filters/Adam_1:0\n",
      "81 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv1/Discriminator.1.Conv1.Biases/Adam:0\n",
      "82 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv1/Discriminator.1.Conv1.Biases/Adam_1:0\n",
      "83 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv2/Discriminator.1.Conv2.Filters/Adam:0\n",
      "84 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv2/Discriminator.1.Conv2.Filters/Adam_1:0\n",
      "85 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv2/Discriminator.1.Conv2.Biases/Adam:0\n",
      "86 WINN/D_optimizer/WINN/layers/Discriminator.1.Conv2/Discriminator.1.Conv2.Biases/Adam_1:0\n",
      "87 WINN/D_optimizer/WINN/layers/Discriminator.2.Shortcut/Discriminator.2.Shortcut.Filters/Adam:0\n",
      "88 WINN/D_optimizer/WINN/layers/Discriminator.2.Shortcut/Discriminator.2.Shortcut.Filters/Adam_1:0\n",
      "89 WINN/D_optimizer/WINN/layers/Discriminator.2.Shortcut/Discriminator.2.Shortcut.Biases/Adam:0\n",
      "90 WINN/D_optimizer/WINN/layers/Discriminator.2.Shortcut/Discriminator.2.Shortcut.Biases/Adam_1:0\n",
      "91 WINN/D_optimizer/WINN/layers/Discriminator.2.N1.offset/Adam:0\n",
      "92 WINN/D_optimizer/WINN/layers/Discriminator.2.N1.offset/Adam_1:0\n",
      "93 WINN/D_optimizer/WINN/layers/Discriminator.2.N1.scale/Adam:0\n",
      "94 WINN/D_optimizer/WINN/layers/Discriminator.2.N1.scale/Adam_1:0\n",
      "95 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv1/Discriminator.2.Conv1.Filters/Adam:0\n",
      "96 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv1/Discriminator.2.Conv1.Filters/Adam_1:0\n",
      "97 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv1/Discriminator.2.Conv1.Biases/Adam:0\n",
      "98 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv1/Discriminator.2.Conv1.Biases/Adam_1:0\n",
      "99 WINN/D_optimizer/WINN/layers/Discriminator.2.N2.offset/Adam:0\n",
      "100 WINN/D_optimizer/WINN/layers/Discriminator.2.N2.offset/Adam_1:0\n",
      "101 WINN/D_optimizer/WINN/layers/Discriminator.2.N2.scale/Adam:0\n",
      "102 WINN/D_optimizer/WINN/layers/Discriminator.2.N2.scale/Adam_1:0\n",
      "103 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv2/Discriminator.2.Conv2.Filters/Adam:0\n",
      "104 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv2/Discriminator.2.Conv2.Filters/Adam_1:0\n",
      "105 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv2/Discriminator.2.Conv2.Biases/Adam:0\n",
      "106 WINN/D_optimizer/WINN/layers/Discriminator.2.Conv2/Discriminator.2.Conv2.Biases/Adam_1:0\n",
      "107 WINN/D_optimizer/WINN/layers/Discriminator.3.N1.offset/Adam:0\n",
      "108 WINN/D_optimizer/WINN/layers/Discriminator.3.N1.offset/Adam_1:0\n",
      "109 WINN/D_optimizer/WINN/layers/Discriminator.3.N1.scale/Adam:0\n",
      "110 WINN/D_optimizer/WINN/layers/Discriminator.3.N1.scale/Adam_1:0\n",
      "111 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv1/Discriminator.3.Conv1.Filters/Adam:0\n",
      "112 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv1/Discriminator.3.Conv1.Filters/Adam_1:0\n",
      "113 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv1/Discriminator.3.Conv1.Biases/Adam:0\n",
      "114 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv1/Discriminator.3.Conv1.Biases/Adam_1:0\n",
      "115 WINN/D_optimizer/WINN/layers/Discriminator.3.N2.offset/Adam:0\n",
      "116 WINN/D_optimizer/WINN/layers/Discriminator.3.N2.offset/Adam_1:0\n",
      "117 WINN/D_optimizer/WINN/layers/Discriminator.3.N2.scale/Adam:0\n",
      "118 WINN/D_optimizer/WINN/layers/Discriminator.3.N2.scale/Adam_1:0\n",
      "119 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv2/Discriminator.3.Conv2.Filters/Adam:0\n",
      "120 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv2/Discriminator.3.Conv2.Filters/Adam_1:0\n",
      "121 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv2/Discriminator.3.Conv2.Biases/Adam:0\n",
      "122 WINN/D_optimizer/WINN/layers/Discriminator.3.Conv2/Discriminator.3.Conv2.Biases/Adam_1:0\n",
      "123 WINN/D_optimizer/WINN/layers/Discriminator.4.N1.offset/Adam:0\n",
      "124 WINN/D_optimizer/WINN/layers/Discriminator.4.N1.offset/Adam_1:0\n",
      "125 WINN/D_optimizer/WINN/layers/Discriminator.4.N1.scale/Adam:0\n",
      "126 WINN/D_optimizer/WINN/layers/Discriminator.4.N1.scale/Adam_1:0\n",
      "127 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv1/Discriminator.4.Conv1.Filters/Adam:0\n",
      "128 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv1/Discriminator.4.Conv1.Filters/Adam_1:0\n",
      "129 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv1/Discriminator.4.Conv1.Biases/Adam:0\n",
      "130 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv1/Discriminator.4.Conv1.Biases/Adam_1:0\n",
      "131 WINN/D_optimizer/WINN/layers/Discriminator.4.N2.offset/Adam:0\n",
      "132 WINN/D_optimizer/WINN/layers/Discriminator.4.N2.offset/Adam_1:0\n",
      "133 WINN/D_optimizer/WINN/layers/Discriminator.4.N2.scale/Adam:0\n",
      "134 WINN/D_optimizer/WINN/layers/Discriminator.4.N2.scale/Adam_1:0\n",
      "135 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv2/Discriminator.4.Conv2.Filters/Adam:0\n",
      "136 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv2/Discriminator.4.Conv2.Filters/Adam_1:0\n",
      "137 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv2/Discriminator.4.Conv2.Biases/Adam:0\n",
      "138 WINN/D_optimizer/WINN/layers/Discriminator.4.Conv2/Discriminator.4.Conv2.Biases/Adam_1:0\n",
      "139 WINN/D_optimizer/WINN/layers/Discriminator.Output/Discriminator.Output.W/Adam:0\n",
      "140 WINN/D_optimizer/WINN/layers/Discriminator.Output/Discriminator.Output.W/Adam_1:0\n",
      "141 WINN/D_optimizer/WINN/layers/Discriminator.Output/Discriminator.Output.b/Adam:0\n",
      "142 WINN/D_optimizer/WINN/layers/Discriminator.Output/Discriminator.Output.b/Adam_1:0\n",
      "143 WINN/S_optimizer/S_step:0\n",
      "144 WINN/S_optimizer/beta1_power:0\n",
      "145 WINN/S_optimizer/beta2_power:0\n",
      "146 WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.W/Adam:0\n",
      "147 WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.W/Adam_1:0\n",
      "148 WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.b/Adam:0\n",
      "149 WINN/S_optimizer/WINN/S_images/Generator.Input/Generator.Input.b/Adam_1:0\n",
      "150 WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters/Adam:0\n",
      "151 WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Filters/Adam_1:0\n",
      "152 WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases/Adam:0\n",
      "153 WINN/S_optimizer/WINN/S_images/Generator.1.Shortcut/Generator.1.Shortcut.Biases/Adam_1:0\n",
      "154 WINN/S_optimizer/WINN/S_images/Generator.1.N1.offset/Adam:0\n",
      "155 WINN/S_optimizer/WINN/S_images/Generator.1.N1.offset/Adam_1:0\n",
      "156 WINN/S_optimizer/WINN/S_images/Generator.1.N1.scale/Adam:0\n",
      "157 WINN/S_optimizer/WINN/S_images/Generator.1.N1.scale/Adam_1:0\n",
      "158 WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters/Adam:0\n",
      "159 WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Filters/Adam_1:0\n",
      "160 WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases/Adam:0\n",
      "161 WINN/S_optimizer/WINN/S_images/Generator.1.Conv1/Generator.1.Conv1.Biases/Adam_1:0\n",
      "162 WINN/S_optimizer/WINN/S_images/Generator.1.N2.offset/Adam:0\n",
      "163 WINN/S_optimizer/WINN/S_images/Generator.1.N2.offset/Adam_1:0\n",
      "164 WINN/S_optimizer/WINN/S_images/Generator.1.N2.scale/Adam:0\n",
      "165 WINN/S_optimizer/WINN/S_images/Generator.1.N2.scale/Adam_1:0\n",
      "166 WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters/Adam:0\n",
      "167 WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Filters/Adam_1:0\n",
      "168 WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases/Adam:0\n",
      "169 WINN/S_optimizer/WINN/S_images/Generator.1.Conv2/Generator.1.Conv2.Biases/Adam_1:0\n",
      "170 WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters/Adam:0\n",
      "171 WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Filters/Adam_1:0\n",
      "172 WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases/Adam:0\n",
      "173 WINN/S_optimizer/WINN/S_images/Generator.2.Shortcut/Generator.2.Shortcut.Biases/Adam_1:0\n",
      "174 WINN/S_optimizer/WINN/S_images/Generator.2.N1.offset/Adam:0\n",
      "175 WINN/S_optimizer/WINN/S_images/Generator.2.N1.offset/Adam_1:0\n",
      "176 WINN/S_optimizer/WINN/S_images/Generator.2.N1.scale/Adam:0\n",
      "177 WINN/S_optimizer/WINN/S_images/Generator.2.N1.scale/Adam_1:0\n",
      "178 WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters/Adam:0\n",
      "179 WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Filters/Adam_1:0\n",
      "180 WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases/Adam:0\n",
      "181 WINN/S_optimizer/WINN/S_images/Generator.2.Conv1/Generator.2.Conv1.Biases/Adam_1:0\n",
      "182 WINN/S_optimizer/WINN/S_images/Generator.2.N2.offset/Adam:0\n",
      "183 WINN/S_optimizer/WINN/S_images/Generator.2.N2.offset/Adam_1:0\n",
      "184 WINN/S_optimizer/WINN/S_images/Generator.2.N2.scale/Adam:0\n",
      "185 WINN/S_optimizer/WINN/S_images/Generator.2.N2.scale/Adam_1:0\n",
      "186 WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters/Adam:0\n",
      "187 WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Filters/Adam_1:0\n",
      "188 WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases/Adam:0\n",
      "189 WINN/S_optimizer/WINN/S_images/Generator.2.Conv2/Generator.2.Conv2.Biases/Adam_1:0\n",
      "190 WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters/Adam:0\n",
      "191 WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Filters/Adam_1:0\n",
      "192 WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases/Adam:0\n",
      "193 WINN/S_optimizer/WINN/S_images/Generator.3.Shortcut/Generator.3.Shortcut.Biases/Adam_1:0\n",
      "194 WINN/S_optimizer/WINN/S_images/Generator.3.N1.offset/Adam:0\n",
      "195 WINN/S_optimizer/WINN/S_images/Generator.3.N1.offset/Adam_1:0\n",
      "196 WINN/S_optimizer/WINN/S_images/Generator.3.N1.scale/Adam:0\n",
      "197 WINN/S_optimizer/WINN/S_images/Generator.3.N1.scale/Adam_1:0\n",
      "198 WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters/Adam:0\n",
      "199 WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Filters/Adam_1:0\n",
      "200 WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases/Adam:0\n",
      "201 WINN/S_optimizer/WINN/S_images/Generator.3.Conv1/Generator.3.Conv1.Biases/Adam_1:0\n",
      "202 WINN/S_optimizer/WINN/S_images/Generator.3.N2.offset/Adam:0\n",
      "203 WINN/S_optimizer/WINN/S_images/Generator.3.N2.offset/Adam_1:0\n",
      "204 WINN/S_optimizer/WINN/S_images/Generator.3.N2.scale/Adam:0\n",
      "205 WINN/S_optimizer/WINN/S_images/Generator.3.N2.scale/Adam_1:0\n",
      "206 WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters/Adam:0\n",
      "207 WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Filters/Adam_1:0\n",
      "208 WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases/Adam:0\n",
      "209 WINN/S_optimizer/WINN/S_images/Generator.3.Conv2/Generator.3.Conv2.Biases/Adam_1:0\n",
      "210 WINN/S_optimizer/WINN/S_images/Generator.OutputN.offset/Adam:0\n",
      "211 WINN/S_optimizer/WINN/S_images/Generator.OutputN.offset/Adam_1:0\n",
      "212 WINN/S_optimizer/WINN/S_images/Generator.OutputN.scale/Adam:0\n",
      "213 WINN/S_optimizer/WINN/S_images/Generator.OutputN.scale/Adam_1:0\n",
      "214 WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Filters/Adam:0\n",
      "215 WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Filters/Adam_1:0\n",
      "216 WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Biases/Adam:0\n",
      "217 WINN/S_optimizer/WINN/S_images/Generator.Output/Generator.Output.Biases/Adam_1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('S_iteration_count_of_batch:', 100)\n",
      "Positive images 50000, negative images 10000, image shape [32, 32, 3]\n",
      "Training stage 3: Cascades training...\n",
      "Discriminator: Cascade 0, iteration 0, all pos 50000, all neg 10000, current iteration 200 (pos 100, neg 100), learning rate 9.99999974738e-05\n",
      "('D_iteration_count_of_batch:', 2)\n",
      "Sampler: Cascade 0, iteration 0, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 0, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 0, Critic 0, time 38.7991421223, D_loss 4.55732917786, D_pos_loss inf, -inf\n",
      "Sampler: Cascade 0, iteration 0, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 0, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 0, Critic 1, time 40.7870919704, D_loss -1.71667671204, D_pos_loss inf, -inf\n",
      "Sampler: Cascade 0, iteration 0, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 0, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 0, Critic 2, time 42.9081771374, D_loss -8.43699455261, D_pos_loss 2.13895964622, 2.97259140015\n",
      "('S_iteration_count_of_batch:', 10)\n",
      "('shape of neg images:', (100, 32, 32, 3))\n",
      "Discriminator: Cascade 0, iteration 1, all pos 50000, all neg 11000, current iteration 400 (pos 200, neg 200), learning rate 9.99999974738e-05\n",
      "('D_iteration_count_of_batch:', 4)\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 1, Critic 0, time 47.0169291496, D_loss -14.551940918, D_pos_loss inf, -inf\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 1, Critic 1, time 50.9217851162, D_loss -19.2105445862, D_pos_loss inf, -inf\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 1, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 1, Critic 2, time 54.9509110451, D_loss -20.9736919403, D_pos_loss 9.86200714111, 13.3625965118\n",
      "('S_iteration_count_of_batch:', 10)\n",
      "('shape of neg images:', (100, 32, 32, 3))\n",
      "Discriminator: Cascade 0, iteration 2, all pos 50000, all neg 12000, current iteration 600 (pos 300, neg 300), learning rate 9.99999974738e-05\n",
      "('D_iteration_count_of_batch:', 6)\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 2, Critic 0, time 60.9519131184, D_loss -23.8233890533, D_pos_loss inf, -inf\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 2, Critic 1, time 66.7758669853, D_loss -27.2384300232, D_pos_loss inf, -inf\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Sampler: Cascade 0, iteration 2, current iteration neg 1000\n",
      "Discriminator: Cascade 0, iteration 2, Critic 2, time 72.8454041481, D_loss -29.9699325562, D_pos_loss 15.6296634674, 20.315158844\n",
      "('S_iteration_count_of_batch:', 10)\n",
      "('shape of neg images:', (100, 32, 32, 3))\n",
      "Discriminator: Cascade 0, iteration 3, all pos 50000, all neg 13000, current iteration 800 (pos 400, neg 400), learning rate 9.99999974738e-05\n",
      "('D_iteration_count_of_batch:', 8)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './data/negativemodel-0.0001_lr-one-batch-gen-disc-improv/cascade_0_iteration_2_count_922.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-48b65a93fb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WINN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-5378894a72d9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess)\u001b[0m\n\u001b[1;32m    198\u001b[0m                         D_pos_iteration_images_path[i * half_batch_size : (i + 1) * half_batch_size]]\n\u001b[1;32m    199\u001b[0m                     D_neg_batch_images = [load_unnormalized_image(path) for path in\n\u001b[0;32m--> 200\u001b[0;31m                         D_neg_iteration_images_path[i * half_batch_size : (i + 1) * half_batch_size]]\n\u001b[0m\u001b[1;32m    201\u001b[0m                     \u001b[0;31m# Normalize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mD_pos_batch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_pos_batch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/winn-new/unsupervised/utils.pyc\u001b[0m in \u001b[0;36mload_unnormalized_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mimage\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     '''\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/AD/kriti/.conda/envs/tf1.3/lib/python2.7/site-packages/scipy/misc/pilutil.pyc\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \"\"\"\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/AD/kriti/.conda/envs/tf1.3/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './data/negativemodel-0.0001_lr-one-batch-gen-disc-improv/cascade_0_iteration_2_count_922.png'"
     ]
    }
   ],
   "source": [
    "# Set dynamic allocation of GPU memory rather than pre-allocation.\n",
    "# Also set soft placement, which means when current GPU does not exist, \n",
    "# it will change into another.\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create computation graph.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Set GPU number and train.\n",
    "    gpu_number = 0\n",
    "    with tf.device(\"/gpu:{0}\".format(gpu_number)):    \n",
    "        # Training session.\n",
    "        with tf.Session(config = config) as sess:\n",
    "            with tf.variable_scope(\"WINN\", reuse = None):\n",
    "                train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "221px",
    "width": "393px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
